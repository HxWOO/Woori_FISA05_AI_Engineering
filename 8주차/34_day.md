## 🤖 머신러닝 기초: 통계부터 딥러닝까지

오늘은 머신러닝의 가장 기본이 되는 통계 개념과 머신러닝의 전반적인 개요, 그리고 딥러닝의 기초까지 학습했습니다. 파이썬 코드를 통해 직접 통계량을 계산하고, 머신러닝의 작동 원리를 구체적으로 살펴보았습니다.

> ### 💡 왜 코드와 함께 배워야 할까?
> 이론으로만 접하는 개념은 쉽게 휘발됩니다. 하지만 코드로 직접 데이터를 만들어보고, 통계량을 계산하고, 모델의 학습 과정을 눈으로 확인하는 과정은 개념을 더욱 명확하고 단단하게 만들어 줍니다.

---

### 🐍 기본 라이브러리 준비

데이터 분석과 머신러닝에 필요한 기본 라이브러리를 가져오는 것으로 학습을 시작했습니다.

```python
import warnings # 경고 메시지 무시
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# numpy 출력 옵션 변경 (지수표기법 방지)
np.set_printoptions(suppress=True)
```

---

### 📊 기술통계 (Descriptive Statistics)

수집한 데이터를 요약, 묘사, 설명하는 통계 기법입니다. 데이터의 전반적인 특징을 파악하는 데 사용됩니다.

#### 1. 중심에 대한 통계 (코드 예제 포함)

데이터의 중심 경향을 나타내는 값들입니다. 실제 도미 데이터를 `pandas`의 DataFrame으로 만들어 각종 통계량을 직접 구해보았습니다.

**- 데이터 준비**
```python
# 도미 데이터 (길이, 무게)
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

df = pd.DataFrame(zip(bream_length, bream_weight), columns=['length', 'weight'])
```

**- 평균, 중앙값, 최빈값 계산**

`describe()` 함수로 기본적인 통계량을 한 번에 확인하고, `value_counts()`로 최빈값을 구했습니다.

```python
# 기본 통계량 (평균-mean, 중앙값-50%)
df.describe()

# 최빈값 (가장 많이 등장하는 값)
df.length.value_counts().head(1)
```

| 구분 | 설명 |
| :--- | :--- |
| **평균 (Mean)** | 모든 값을 더해 개수로 나눈 값. `describe()`의 `mean`에 해당합니다. |
| **중앙값 (Median)** | 데이터를 정렬했을 때 중앙에 위치하는 값. `describe()`의 `50%`에 해당합니다. |
| **최빈값 (Mode)** | 가장 자주 나타나는 값. `value_counts()`로 확인 가능합니다. |

#### 2. 산포에 대한 통계 (코드 예제 포함)

데이터가 중심으로부터 얼마나 흩어져 있는지를 나타냅니다.

**- 분산과 표준편차 계산**
```python
# 1. 편차 (Deviation)
mean_ = df.length.mean()
deviation = df.length - mean_

# 2. 변동 (Variation) - 편차의 제곱합
variation = deviation**2

# 3. 분산 (Variance)
variance = sum(variation) / len(variation)

# 4. 표준편차 (Standard Deviation)
std = np.sqrt(variance)
```
> **💡 표본 표준편차**
> 우리가 가진 데이터는 세상의 모든 도미(모집단)가 아닌 일부(표본)입니다. 따라서 표본 데이터로 모집단을 추정할 때는 자유도(n-1)를 적용하여 표준편차를 계산하는 것이 더 정확합니다. `pandas`의 `std()`는 기본적으로 표본 표준편차를 계산해줍니다.

#### 3. 정규화와 표준화 (코드 예제 포함)

데이터의 단위를 맞춰주어(스케일링) 모델이 더 잘 학습할 수 있도록 돕습니다.

**- 표준화 (Standardization, Z-score)**

데이터에서 평균을 빼고 표준편차로 나누어, 평균 0, 표준편차 1인 분포로 변환합니다.

```python
# 도미 길이(length)를 표준화
z_length = (df.length - df.length.mean()) / np.std(df.length)

# 도미 무게(weight)를 표준화
z_weight = (df.weight - df.weight.mean()) / np.std(df.weight)

# 표준화된 데이터 확인
new_df = pd.DataFrame(zip(z_length, z_weight), columns=['length_z', 'weight_z'])
new_df.describe()
```
결과를 보면 `mean`은 0에 가깝고, `std`는 1에 가까운 값으로 변환된 것을 볼 수 있습니다.

---

### 🧠 머신러닝 & 딥러닝 기초

#### 1. 머신러닝 개요

- **정의:** 명시적인 프로그래밍 없이, 컴퓨터가 데이터와 경험을 통해 학습하여 처리 능력을 향상시키는 연구 분야입니다.
- **기본 모형:** $\hat Y=f(x)$ (데이터 x를 가장 잘 설명하는 함수 f를 찾는 과정)
- **종류:** 지도학습, 비지도학습, 강화학습 등

#### 2. 딥러닝: 퍼셉트론으로 맛보기

딥러닝의 기본 단위인 뉴런(퍼셉트론)이 어떻게 학습하는지 간단한 코드로 구현해보았습니다.

**- 퍼셉트론 학습 과정**

`x=1`이 입력되면 `y=0`을 출력하는 아주 간단한 뉴런을 만들어봅니다.

```python
# y = wx + b
w = np.random.uniform(0, 1) # 가중치(w)는 랜덤 값으로 시작
x = 1
y = 0
eta = 0.1  # 학습률 (learning rate)

for i in range(10): # 10번 반복 학습
    output = w * x      # 예측값 계산
    error = y - output  # 실제값과 예측값의 차이(오차) 계산
    
    # 경사 하강법: 오차의 반대 방향으로 가중치를 업데이트
    w = w + (x * error) * eta
    
    print(f"{i+1}회차 가중치: {w:.4f}, 오차: {error:.4f}")

print(f"\n최종 예측값: {w*x:.4f}")
```
학습이 반복될수록 가중치(w)가 점차 갱신되면서 오차(error)가 0에 가까워지고, 최종 예측값이 목표인 0에 수렴하는 것을 확인할 수 있습니다.

---

### ✨ 오늘 배운 것 요약

- **기술통계**는 `pandas`의 `describe()`와 같은 함수로 쉽게 계산할 수 있으며, 데이터의 특징을 파악하는 첫걸음입니다.
- **표준화**는 서로 다른 단위(scale)를 가진 데이터들을 비교 가능한 상태로 만들어 머신러닝 모델의 성능을 높여줍니다.
- **머신러닝**은 데이터로부터 규칙(함수 f)을 학습하는 과정이며, **딥러닝**은 인공신경망을 기반으로 합니다.
- 가장 단순한 뉴런(퍼셉트론)도 **예측 → 오차 계산 → 가중치 업데이트**의 과정을 반복하며 정답에 가까워지도록 학습합니다.