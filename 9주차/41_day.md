# ğŸ—ºï¸ ë¹„ì§€ë„ í•™ìŠµê³¼ PCA

## ğŸ¯ ëª©ì°¨
1. [ì°¨ì›ì˜ ì €ì£¼ (The Curse of Dimensionality)](#1-ğŸ§-ì°¨ì›ì˜-ì €ì£¼-the-curse-of-dimensionality)
2. [ì£¼ì„±ë¶„ ë¶„ì„ (Principal Component Analysis, PCA)](#2-ğŸ§ -ì£¼ì„±ë¶„-ë¶„ì„-principal-component-analysis-pca)
3. [PCA ì‹¤ìŠµ: ì™€ì¸ ë°ì´í„°ì…‹](#3-ğŸš€-pca-ì‹¤ìŠµ-ì™€ì¸-ë°ì´í„°ì…‹)
4. [ìš”ì•½ ì •ë¦¬](#4-âœï¸-ìš”ì•½-ì •ë¦¬)

---

## 1. ğŸ§ ì°¨ì›ì˜ ì €ì£¼ (The Curse of Dimensionality)

- ë”¥ëŸ¬ë‹ì—ì„œ ì¸µì„ ê¹Šê²Œ ìŒ“ì•„ ì°¨ì›ì´ ì»¤ì§€ë©´, ë” ë³µì¡í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í–ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì°¨ì›ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ì»¤ì§€ë©´ í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ê°€ ì°¨ì›ì˜ ìˆ˜ë³´ë‹¤ ë¶€ì¡±í•´ì ¸ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” í˜„ìƒì´ ë°œìƒí–ˆë‹¤.
- ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡ ê° ì°¨ì› ë‚´ì—ì„œ í•™ìŠµí•  ë°ì´í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“œëŠ” **í¬ì†Œ(sparse) í˜„ìƒ**ì´ ë°œìƒí•œë‹¤.
- ì´ëŠ” í•™ìŠµì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ” ì›ì¸ì´ ë˜ë©°, ì—°ì‚°ëŸ‰ ë˜í•œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ íš¨ìœ¨ì„±ì„ ë–¨ì–´ëœ¨ë¦°ë‹¤.

**í•´ê²°ì±…:**
- ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤.
- ë°ì´í„°ë¥¼ ë” ë§ì´ í™•ë³´í•œë‹¤.

![Curse of Dimensionality](https://images.deepai.org/glossary-terms/curse-of-dimensionality-61461.jpg)

---

## 2. ğŸ§  ì£¼ì„±ë¶„ ë¶„ì„ (Principal Component Analysis, PCA)

- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” **ì°¨ì› ì¶•ì†Œ ê¸°ë²•** ì¤‘ í•˜ë‚˜ë‹¤.
- ì› ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
- ì—¬ëŸ¬ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬, ì´ë¥¼ ëŒ€í‘œí•˜ëŠ” **ì£¼ì„±ë¶„(Principal Component)** ì„ ì¶”ì¶œí•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤. ì´ ê³¼ì •ì—ì„œ ì •ë³´ ìœ ì‹¤ì„ ìµœì†Œí™”í•œë‹¤.

![PCA](https://t1.daumcdn.net/cfile/tistory/99CB343359F2DA5E07)

### PCAì˜ ì›ë¦¬

- ì„ í˜•ëŒ€ìˆ˜ ê´€ì ì—ì„œ PCAëŠ” ì…ë ¥ ë°ì´í„°ì˜ **ê³µë¶„ì‚° í–‰ë ¬(Covariance Matrix)** ì„ **ê³ ìœ ê°’ ë¶„í•´(Eigenvalue Decomposition)** í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
- ì´ë•Œ ì–»ì–´ì§€ëŠ” **ê³ ìœ ë²¡í„°(Eigenvector)** ê°€ PCAì˜ ì£¼ì„±ë¶„ ë²¡í„°ê°€ ë˜ë©°, ì´ëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì´ ê°€ì¥ í° ë°©í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.

### PCA ìˆ˜í–‰ ë‹¨ê³„

1.  í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ **ë¶„ì‚°ì´ ìµœëŒ€ì¸ ì¶•**ì„ ì°¾ëŠ”ë‹¤.
2.  ì²« ë²ˆì§¸ ì¶•ê³¼ **ì§êµ(orthogonal)** í•˜ë©´ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ë‘ ë²ˆì§¸ ì¶•ì„ ì°¾ëŠ”ë‹¤.
3.  ì´ì „ ì¶•ë“¤ê³¼ ì§êµí•˜ë©´ì„œ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ë‹¤ìŒ ì¶•ì„ ì°¾ëŠ”ë‹¤.
4.  ë°ì´í„°ì…‹ì˜ ì°¨ì›(íŠ¹ì„± ìˆ˜)ë§Œí¼ì˜ ì¶•ì„ ì°¾ì„ ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

> **ì¶”ì²œ ì°¸ê³  ìë£Œ:** [PCA, ì œëŒ€ë¡œ ì´í•´í•˜ê¸°](https://angeloyeo.github.io/2019/07/27/PCA.html)

---

## 3. ğŸš€ PCA ì‹¤ìŠµ: ì™€ì¸ ë°ì´í„°ì…‹

### ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
wine = load_wine()
wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
wine_df['target'] = wine.target

# ë°ì´í„°ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
X = wine_df.drop('target', axis=1)
y = wine_df['target']

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### PCA ì ìš© ë° ì‹œê°í™”

```python
from sklearn.decomposition import PCA

# PCA ëª¨ë¸ ìƒì„± (2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# PCA ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(8, 6))
for i in range(len(wine.target_names)):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=wine.target_names[i])

plt.title('PCA of Wine Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid(True)
plt.show()
```

### ì„¤ëª…ëœ ë¶„ì‚° (Explained Variance)

```python
# ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨
print("Explained variance ratio:", pca.explained_variance_ratio_)
# [0.36198848 0.1920749 ]

# ë‘ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„¤ëª…ë˜ëŠ” ì´ ë¶„ì‚°
print("Sum of explained variance ratio:", np.sum(pca.explained_variance_ratio_))
# 0.5540633847175112
```
- ì²« ë²ˆì§¸ ì£¼ì„±ë¶„(PC1)ì€ ì•½ 36.2%ì˜ ë¶„ì‚°ì„, ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„(PC2)ì€ ì•½ 19.2%ì˜ ë¶„ì‚°ì„ ì„¤ëª…í–ˆë‹¤.
- 2ê°œì˜ ì£¼ì„±ë¶„ë§Œìœ¼ë¡œ ì „ì²´ ë°ì´í„° ë¶„ì‚°ì˜ ì•½ 55.4%ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆì—ˆë‹¤.

### ì›ë³¸ ë°ì´í„°ì™€ PCA ë³€í™˜ ë°ì´í„°ì˜ ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµ

#### ë¡œì§€ìŠ¤í‹± íšŒê·€

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# ì›ë³¸ ë°ì´í„°ë¡œ í•™ìŠµ
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
lr_original = LogisticRegression()
lr_original.fit(X_train, y_train)
pred_original = lr_original.predict(X_test)
acc_original = accuracy_score(y_test, pred_original)
print(f"ì›ë³¸ ë°ì´í„° ì •í™•ë„: {acc_original:.4f}") # 1.0000

# PCA ë°ì´í„°ë¡œ í•™ìŠµ
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)
lr_pca = LogisticRegression()
lr_pca.fit(X_train_pca, y_train_pca)
pred_pca = lr_pca.predict(X_test_pca)
acc_pca = accuracy_score(y_test_pca, pred_pca)
print(f"PCA ë³€í™˜ ë°ì´í„° ì •í™•ë„: {acc_pca:.4f}") # 0.9815
```
- **ê²°ê³¼:** 13ê°œì˜ íŠ¹ì„±ì„ ëª¨ë‘ ì‚¬ìš©í–ˆì„ ë•Œ ì •í™•ë„ëŠ” 1.0ì´ì—ˆì§€ë§Œ, 2ê°œì˜ ì£¼ì„±ë¶„ë§Œ ì‚¬ìš©í–ˆì„ ë•Œë„ ì•½ 0.98ì˜ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤. ì°¨ì›ì„ í¬ê²Œ ì¤„ì˜€ìŒì—ë„ ì„±ëŠ¥ ì €í•˜ê°€ ê±°ì˜ ì—†ìŒì„ í™•ì¸í–ˆë‹¤.

#### ê²°ì • íŠ¸ë¦¬

```python
from sklearn.tree import DecisionTreeClassifier

# ì›ë³¸ ë°ì´í„°ë¡œ í•™ìŠµ
dt_original = DecisionTreeClassifier(random_state=42)
dt_original.fit(X_train, y_train)
pred_original_dt = dt_original.predict(X_test)
acc_original_dt = accuracy_score(y_test, pred_original_dt)
print(f"ì›ë³¸ ë°ì´í„° ì •í™•ë„ (DT): {acc_original_dt:.4f}") # 0.9444

# PCA ë°ì´í„°ë¡œ í•™ìŠµ
dt_pca = DecisionTreeClassifier(random_state=42)
dt_pca.fit(X_train_pca, y_train_pca)
pred_pca_dt = dt_pca.predict(X_test_pca)
acc_pca_dt = accuracy_score(y_test_pca, pred_pca_dt)
print(f"PCA ë³€í™˜ ë°ì´í„° ì •í™•ë„ (DT): {acc_pca_dt:.4f}") # 0.9259
```
- **ê²°ê³¼:** ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ì—ì„œë„ PCAë¥¼ í†µí•´ ì°¨ì›ì„ ì¶•ì†Œí•œ ë°ì´í„°ê°€ ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.

---

## 4. âœï¸ ìš”ì•½ ì •ë¦¬

- **ì°¨ì›ì˜ ì €ì£¼**ëŠ” ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” í¬ì†Œì„± ë¬¸ì œë¡œ, ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì´ ëœë‹¤.
- **PCA**ëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì£¼ì„±ë¶„ì„ ì¶”ì¶œí•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•ì´ë‹¤.
- ì™€ì¸ ë°ì´í„°ì…‹ ì‹¤ìŠµ ê²°ê³¼, 13ê°œ íŠ¹ì„±ì„ ë‹¨ 2ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ì¤„ì˜€ìŒì—ë„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì •í™•ë„ê°€ ê±°ì˜ ìœ ì§€ë˜ì–´ PCAì˜ íš¨ìœ¨ì„±ì„ í™•ì¸í–ˆë‹¤.
