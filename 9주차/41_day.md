# ğŸ—ºï¸ ë¹„ì§€ë„ í•™ìŠµê³¼ PCA, ê·¸ë¦¬ê³  ë§¤ë‹ˆí´ë“œ í•™ìŠµ

## ğŸ¯ ëª©ì°¨
1. [ì°¨ì›ì˜ ì €ì£¼ (The Curse of Dimensionality)](#1-ğŸ§-ì°¨ì›ì˜-ì €ì£¼-the-curse-of-dimensionality)
2. [ì£¼ì„±ë¶„ ë¶„ì„ (Principal Component Analysis, PCA)](#2-ğŸ§ -ì£¼ì„±ë¶„-ë¶„ì„-principal-component-analysis-pca)
3. [PCA ì‹¤ìŠµ: ë¶“ê½ƒ ë°ì´í„°ì…‹](#3-ğŸš€-pca-ì‹¤ìŠµ-ë¶“ê½ƒ-ë°ì´í„°ì…‹)
4. [PCA ì‹¤ìŠµ: ì´ë¯¸ì§€ ë°ì´í„° (MNIST)](#4-ğŸ–¼ï¸-pca-ì‹¤ìŠµ-ì´ë¯¸ì§€-ë°ì´í„°-mnist)
5. [ë§¤ë‹ˆí´ë“œ í•™ìŠµ (Manifold Learning)ê³¼ t-SNE](#5-ğŸ¨-ë§¤ë‹ˆí´ë“œ-í•™ìŠµmanifold-learningê³¼-t-sne)
6. [PCA ì‘ìš©: ê³ ìœ  ì–¼êµ´ (Eigenface)](#6-ğŸ§‘-pca-ì‘ìš©-ê³ ìœ -ì–¼êµ´-eigenface)
7. [PCA ì‘ìš©: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ (íŒŒì´í”„ë¼ì¸)](#7-ğŸ©º-pca-ì‘ìš©-ìœ„ìŠ¤ì½˜ì‹ -ìœ ë°©ì•”-ë°ì´í„°ì…‹-íŒŒì´í”„ë¼ì¸)
8. [ìš”ì•½ ì •ë¦¬](#8-âœï¸-ìš”ì•½-ì •ë¦¬)

---

## 1. ğŸ§ ì°¨ì›ì˜ ì €ì£¼ (The Curse of Dimensionality)

- ë”¥ëŸ¬ë‹ì—ì„œ ì¸µì„ ê¹Šê²Œ ìŒ“ì•„ ì°¨ì›ì´ ì»¤ì§€ë©´, ë” ë³µì¡í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í–ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì°¨ì›ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ì»¤ì§€ë©´ í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ê°€ ì°¨ì›ì˜ ìˆ˜ë³´ë‹¤ ë¶€ì¡±í•´ì ¸ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” í˜„ìƒì´ ë°œìƒí–ˆë‹¤.
- ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡ ê° ì°¨ì› ë‚´ì—ì„œ í•™ìŠµí•  ë°ì´í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“œëŠ” **í¬ì†Œ(sparse) í˜„ìƒ**ì´ ë°œìƒí•œë‹¤.
- ì´ëŠ” í•™ìŠµì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ” ì›ì¸ì´ ë˜ë©°, ì—°ì‚°ëŸ‰ ë˜í•œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ íš¨ìœ¨ì„±ì„ ë–¨ì–´ëœ¨ë¦°ë‹¤.

**í•´ê²°ì±…:**
- ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤.
- ë°ì´í„°ë¥¼ ë” ë§ì´ í™•ë³´í•œë‹¤.

![Curse of Dimensionality](https://images.deepai.org/glossary-terms/curse-of-dimensionality-61461.jpg)

---

## 2. ğŸ§  ì£¼ì„±ë¶„ ë¶„ì„ (Principal Component Analysis, PCA)

- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” **ì°¨ì› ì¶•ì†Œ ê¸°ë²•** ì¤‘ í•˜ë‚˜ë‹¤.
- ì› ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
- ì—¬ëŸ¬ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬, ì´ë¥¼ ëŒ€í‘œí•˜ëŠ” **ì£¼ì„±ë¶„(Principal Component)** ì„ ì¶”ì¶œí•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤. ì´ ê³¼ì •ì—ì„œ ì •ë³´ ìœ ì‹¤ì„ ìµœì†Œí™”í•œë‹¤.

![PCA](https://t1.daumcdn.net/cfile/tistory/99CB343359F2DA5E07)

### PCAì˜ ì›ë¦¬

- ì„ í˜•ëŒ€ìˆ˜ ê´€ì ì—ì„œ PCAëŠ” ì…ë ¥ ë°ì´í„°ì˜ **ê³µë¶„ì‚° í–‰ë ¬(Covariance Matrix)** ì„ **ê³ ìœ ê°’ ë¶„í•´(Eigenvalue Decomposition)** í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
- ì´ë•Œ ì–»ì–´ì§€ëŠ” **ê³ ìœ ë²¡í„°(Eigenvector)** ê°€ PCAì˜ ì£¼ì„±ë¶„ ë²¡í„°ê°€ ë˜ë©°, ì´ëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì´ ê°€ì¥ í° ë°©í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.

### PCA ìˆ˜í–‰ ë‹¨ê³„

1.  í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ **ë¶„ì‚°ì´ ìµœëŒ€ì¸ ì¶•**ì„ ì°¾ëŠ”ë‹¤.
2.  ì²« ë²ˆì§¸ ì¶•ê³¼ **ì§êµ(orthogonal)** í•˜ë©´ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ë‘ ë²ˆì§¸ ì¶•ì„ ì°¾ëŠ”ë‹¤.
3.  ì´ì „ ì¶•ë“¤ê³¼ ì§êµí•˜ë©´ì„œ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ë‹¤ìŒ ì¶•ì„ ì°¾ëŠ”ë‹¤.
4.  ë°ì´í„°ì…‹ì˜ ì°¨ì›(íŠ¹ì„± ìˆ˜)ë§Œí¼ì˜ ì¶•ì„ ì°¾ì„ ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

> **ì¶”ì²œ ì°¸ê³  ìë£Œ:** [PCA, ì œëŒ€ë¡œ ì´í•´í•˜ê¸°](https://angeloyeo.github.io/2019/07/27/PCA.html)

---

## 3. ğŸš€ PCA ì‹¤ìŠµ: ë¶“ê½ƒ ë°ì´í„°ì…‹

4ì°¨ì›ì˜ ë¶“ê½ƒ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì‹œê°í™”í•˜ê³ , ì›ë³¸ ë°ì´í„°ì™€ PCA ë³€í™˜ ë°ì´í„°ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤.

### ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§

- PCAëŠ” ì—¬ëŸ¬ ì†ì„±ì˜ ìŠ¤ì¼€ì¼ì— ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ, **PCA ì ìš© ì „ ê° ì†ì„±ì„ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜**í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- `StandardScaler`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì†ì„± ê°’ì„ í‰ê·  0, ë¶„ì‚° 1ì¸ í‘œì¤€ ì •ê·œ ë¶„í¬ë¡œ ë³€í™˜í•œë‹¤.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['target'] = iris.target

# ì›ë³¸ ë°ì´í„° ì‹œê°í™” (sepal length vs sepal width)
markers=['^','s','o']
for i, marker in enumerate(markers):
    x_axis_data = iris_df[iris_df['target']==i]['sepal_length']
    y_axis_data = iris_df[iris_df['target']==i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])

plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
iris_scaled = StandardScaler().fit_transform(iris_df.iloc[:,:-1])
```

### PCA ì ìš© ë° ì‹œê°í™”

```python
# 2ì°¨ì›ìœ¼ë¡œ PCA ë³€í™˜
pca = PCA(n_components=2)
iris_pca = pca.fit_transform(iris_scaled)

# PCA ë°ì´í„°í”„ë ˆì„ ìƒì„±
pca_columns=['pca_component_1','pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns)
irisDF_pca['target'] = iris.target

# PCA ê²°ê³¼ ì‹œê°í™”
markers=['^','s','o']
for i, marker in enumerate(markers):
    x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1']
    y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])

plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
plt.show()
```

### ì›ë³¸ ë°ì´í„°ì™€ PCA ë³€í™˜ ë°ì´í„°ì˜ ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµ

```python
# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ì‚¬ìš©
rcf = RandomForestClassifier(random_state=156)

# ì›ë³¸ ë°ì´í„°ë¡œ êµì°¨ ê²€ì¦
scores = cross_val_score(rcf, iris.data, iris.target, scoring='accuracy', cv=3)
print('ì›ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:', scores)
print('ì›ë³¸ ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores))

# PCA ë°ì´í„°ë¡œ êµì°¨ ê²€ì¦
pca_X = irisDF_pca[['pca_component_1','pca_component_2']]
scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3)
print('PCA ë³€í™˜ ë°ì´í„° êµì°¨ ê²€ì¦ ê°œë³„ ì •í™•ë„:', scores_pca)
print('PCA ë³€í™˜ ë°ì´í„° í‰ê·  ì •í™•ë„:', np.mean(scores_pca))
```
- **ê²°ê³¼:** 4ê°œì˜ íŠ¹ì„±ì„ 2ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ì¤„ì˜€ìŒì—ë„, ì›ë³¸ ë°ì´í„°ì˜ í‰ê·  ì •í™•ë„(ì•½ 0.96)ì™€ PCA ë³€í™˜ ë°ì´í„°ì˜ í‰ê·  ì •í™•ë„(ì•½ 0.89) ì‚¬ì´ì— í° ì°¨ì´ê°€ ë‚˜ì§€ ì•Šì•˜ë‹¤. ì´ëŠ” PCAê°€ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©° íš¨ê³¼ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí–ˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

---

## 4. ğŸ–¼ï¸ PCA ì‹¤ìŠµ: ì´ë¯¸ì§€ ë°ì´í„° (MNIST)

784ì°¨ì›(28x28 í”½ì…€)ì˜ MNIST ìˆ«ì ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ PCAë¥¼ ì´ìš©í•´ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•í•˜ê³  ë³µì›í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•œë‹¤.

### PCA ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

```python
import numpy as np
import scipy
import scipy.stats
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
MNIST = fetch_openml('mnist_784', version=1)
images = MNIST['data'].to_numpy().astype(np.double) / 255.

# 1. ì •ê·œí™” (í‰ê·  ì œê±°)
def normalize(X):
    mu = np.mean(X, axis=0)
    Xbar = X - mu
    return Xbar, mu

# 2. ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’/ê³ ìœ ë²¡í„° ê³„ì‚°
def eig(S):
    eig_vals, eig_vecs = np.linalg.eig(S)
    sort_indices = np.argsort(eig_vals)[::-1] # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    return eig_vals[sort_indices], eig_vecs[:, sort_indices]

# 3. ì£¼ì„±ë¶„ìœ¼ë¡œ ë°ì´í„° ë³µì› (ì‚¬ì˜)
def reconstruct(X, PC):
    return (X @ PC) @ PC.T

# PCA ì•Œê³ ë¦¬ì¦˜
def PCA(images, num_components, num_data=1000):
    X = images[:num_data]
    N, D = X.shape
    X_normalized, mean = normalize(X)
    S = (X_normalized.T @ X_normalized) / N
    eig_vals, eig_vecs = eig(S)
    principal_vals, principal_components = np.real(eig_vals[:num_components]), np.real(eig_vecs[:,:num_components])
    reconst_X = reconstruct(X_normalized, principal_components) + mean
    return reconst_X, mean, principal_vals, principal_components
```

### ì£¼ì„±ë¶„ ê°œìˆ˜ì— ë”°ë¥¸ ì´ë¯¸ì§€ ë³µì› ë° MSE

```python
# MSE ê³„ì‚° í•¨ìˆ˜
def mse(predict, actual):
    return np.square(predict - actual).sum(axis=1).mean()

loss = []
reconstructions = []
X = images[:1000]

for num_component in range(1, 100, 5):
    reconst, _, _, _ = PCA(X, num_component)
    error = mse(reconst, X)
    reconstructions.append(reconst)
    print(f'n = {num_component:d}, reconstruction_error = {error:f}')
    loss.append((num_component, error))

loss = np.asarray(loss)

# MSE ì‹œê°í™”
fig, ax = plt.subplots()
ax.plot(loss[:,0], loss[:,1]);
ax.axhline(10, linestyle='--', color='r', linewidth=2)
ax.xaxis.set_ticks(np.arange(1, 100, 5));
ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');
```
- **ê²°ê³¼:** ì£¼ì„±ë¶„ ê°œìˆ˜ê°€ ì•½ 41ê°œì¼ ë•Œ MSEê°€ 10 ì´í•˜ë¡œ ë–¨ì–´ì§€ë©° ê¸°ìš¸ê¸°ê°€ ì™„ë§Œí•´ì§„ë‹¤. ì´ëŠ” 784ì°¨ì›ì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì•½ 41ê°œì˜ ì£¼ì„±ë¶„ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ë³µì› ê°€ëŠ¥í•¨ì„ ì˜ë¯¸í•œë‹¤.

---

## 5. ğŸ¨ ë§¤ë‹ˆí´ë“œ í•™ìŠµ(Manifold Learning)ê³¼ t-SNE

- **ë§¤ë‹ˆí´ë“œ(Manifold, ë‹¤ì–‘ì²´)** ëŠ” êµ­ì†Œì ìœ¼ë¡œ ìœ í´ë¦¬ë“œ ê³µê°„ê³¼ ë‹®ì€ ìœ„ìƒ ê³µê°„ì„ ì˜ë¯¸í•œë‹¤.
- ë§¤ë‹ˆí´ë“œ í•™ìŠµì€ ê³ ì°¨ì› ë°ì´í„°ê°€ ì‹¤ì œë¡œëŠ” ì €ì°¨ì›ì˜ ë§¤ë‹ˆí´ë“œì— ì„ë² ë”©ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ì´ ì €ì°¨ì› êµ¬ì¡°ë¥¼ ì°¾ì•„ë‚´ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì´ë‹¤.
- **t-SNE(t-Distributed Stochastic Neighbor Embedding)** ëŠ” ì‹œê°í™”ë¥¼ ëª©ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë§¤ë‹ˆí´ë“œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ê³ ì°¨ì› ê³µê°„ì—ì„œ ë¹„ìŠ·í•œ ë°ì´í„° êµ¬ì¡°ëŠ” ì €ì°¨ì› ê³µê°„ì—ì„œë„ ê°€ê¹ê²Œ ìœ ì§€ë˜ë„ë¡ ë§µí•‘í•œë‹¤.

### t-SNE ì‹¤ìŠµ: ìˆ«ì ë°ì´í„°ì…‹

```python
from sklearn.datasets import load_digits
from sklearn.manifold import TSNE
import matplotlib.patheffects as PathEffects

# ë°ì´í„° ë¡œë“œ
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# t-SNE ëª¨ë¸ ìƒì„± ë° ë³€í™˜
tsne = TSNE(n_components=2, init='pca', random_state=123)
X_digits_tsne = tsne.fit_transform(X_digits)

# t-SNE ê²°ê³¼ ì‹œê°í™”
def plot_projection(x, colors):
  f = plt.figure(figsize=(8,8))
  ax = plt.subplot(aspect='equal')
  for i in range(10):
    plt.scatter(x[colors==i, 0],
                x[colors==i, 1])
  for i in range(10):
    xtext, ytext = np.median(x[colors==i, :], axis=0)
    txt = ax.text(xtext, ytext, str(i), fontsize=24)
    txt.set_path_effects([
        PathEffects.Stroke(linewidth=5, foreground="w"),
        PathEffects.Normal()])

plot_projection(X_digits_tsne, y_digits)
plt.show()
```
- **ê²°ê³¼:** t-SNEë¥¼ í†µí•´ 64ì°¨ì›ì˜ ìˆ«ì ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•œ ê²°ê³¼, ê°™ì€ ìˆ«ìë¼ë¦¬ ì˜ êµ°ì§‘ì„ ì´ë£¨ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

---

## 6. ğŸ§‘ PCA ì‘ìš©: ê³ ìœ  ì–¼êµ´ (Eigenface)

- PCAë¥¼ ì–¼êµ´ ì´ë¯¸ì§€ì— ì ìš©í•˜ì—¬ **ê³ ìœ  ì–¼êµ´(Eigenface)** ì´ë¼ëŠ” ì£¼ìš” íŠ¹ì„±ì„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.
- ì´ëŠ” ì–¼êµ´ ì¸ì‹ ì‹œìŠ¤í…œ ë“±ì—ì„œ í™œìš©ëœë‹¤.

```python
from sklearn.datasets import fetch_olivetti_faces

# ë°ì´í„° ë¡œë“œ
faces_all = fetch_olivetti_faces()
K = 20 # íŠ¹ì • ì¸ë¬¼ ì„ íƒ
faces = faces_all.images[faces_all.target == K]

# ì–¼êµ´ ì´ë¯¸ì§€ ì‹œê°í™”
N = 2
M = 5
fig = plt.figure(figsize=(10, 5))
plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)
for n in range(N*M):
    ax = fig.add_subplot(N, M, n+1)
    ax.imshow(faces[n], cmap=plt.cm.bone)
    ax.grid(False)
    ax.xaxis.set_ticks([])
    ax.yaxis.set_ticks([])
plt.tight_layout()
plt.show()
```

---

## 7. ğŸ©º PCA ì‘ìš©: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ (íŒŒì´í”„ë¼ì¸)

- `StandardScaler`, `PCA`, `LogisticRegression`ì„ `make_pipeline`ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬, ì°¨ì› ì¶•ì†Œ, ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ í•œ ë²ˆì— íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
- 30ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ 2ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•œë‹¤.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, test_size=0.2, random_state=42
)

# íŒŒì´í”„ë¼ì¸ ìƒì„± ë° í•™ìŠµ
pipe = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression())
pipe.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
print("Train Score:", pipe.score(X_train, y_train))
print("Test Score:", pipe.score(X_test, y_test))

# PCA ë³€í™˜ ë°ì´í„° í™•ì¸
pca = pipe.named_steps['pca']
X_pca = pca.transform(StandardScaler().fit_transform(cancer.data))
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
print(pca_df.head())
```
- **ê²°ê³¼:** 30ê°œì˜ íŠ¹ì„±ì„ 2ê°œë¡œ ì¤„ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì•½ 97.4%ì˜ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤. íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆë‹¤.

---

## 8. âœï¸ ìš”ì•½ ì •ë¦¬

- **ì°¨ì›ì˜ ì €ì£¼**ëŠ” ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” í¬ì†Œì„± ë¬¸ì œë¡œ, ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì´ ëœë‹¤.
- **PCA**ëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì£¼ì„±ë¶„ì„ ì¶”ì¶œí•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ì„ í˜• ê¸°ë²•ì´ë‹¤.
- **ë§¤ë‹ˆí´ë“œ í•™ìŠµ(t-SNE)** ì€ ë°ì´í„°ì˜ êµ­ì†Œì  êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©° ì‹œê°í™”ì— ìœ ìš©í•œ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì´ë‹¤.
- ë¶“ê½ƒ, MNIST, ìœ ë°©ì•” ë°ì´í„°ì…‹ ì‹¤ìŠµ ê²°ê³¼, PCAë¥¼ í†µí•´ íŠ¹ì„±ì˜ ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì—¬ë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ì§€ ì•Šê±°ë‚˜, ì‹œê°í™”ë¥¼ í†µí•´ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì„±ê³µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆì—ˆë‹¤.
- **íŒŒì´í”„ë¼ì¸** ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì „ì²˜ë¦¬, ì°¨ì› ì¶•ì†Œ, ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì²´ê³„ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. ì„±ê³µ! ğŸ‰