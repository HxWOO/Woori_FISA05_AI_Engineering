# ğŸš€ ì§€ë„í•™ìŠµ - íšŒê·€ ë¶„ì„ (Regression Analysis)

## ğŸ¯ ëª©ì°¨
- [ì§€ë„í•™ìŠµ - íšŒê·€ ë¶„ì„ (Regression Analysis)](#-ì§€ë„í•™ìŠµ---íšŒê·€-ë¶„ì„-regression-analysis)
- [ì°¸ê³  : ì„ í˜• ëª¨ë¸(Linear Models)](#-ì°¸ê³ --ì„ í˜•-ëª¨ë¸linear-models)
- [1. ë°ì´í„° ìƒì„± ë° ë‹¨ìˆœ íšŒê·€ ğŸš€](#1-ë°ì´í„°-ìƒì„±-ë°-ë‹¨ìˆœ-íšŒê·€-)
- [2. íšŒê·€ ëª¨ë¸ í‰ê°€ ì§€í‘œ ğŸ§](#2-íšŒê·€-ëª¨ë¸-í‰ê°€-ì§€í‘œ-)
- [3. ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ì‹¤ìŠµ ğŸ ](#3-ìº˜ë¦¬í¬ë‹ˆì•„-ì£¼íƒ-ê°€ê²©-ì˜ˆì¸¡-ì‹¤ìŠµ-)
  - [ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰](#ë°ì´í„°-ë¡œë“œ-ë°-íƒìƒ‰)
  - [ì‹¤í—˜ 1: Baseline ëª¨ë¸](#ì‹¤í—˜-1-baseline-ëª¨ë¸)
  - [ì‹¤í—˜ 2: Scaling](#ì‹¤í—˜-2-scaling)
  - [ì‹¤í—˜ 3: Feature Selection](#ì‹¤í—˜-3-feature-selection)
  - [ì‹¤í—˜ 4: ê·œì œê°€ ìˆëŠ” ëª¨ë¸ (Ridge, Lasso)](#ì‹¤í—˜-4-ê·œì œê°€-ìˆëŠ”-ëª¨ë¸-ridge-lasso)
  - [ì‹¤í—˜ 5: AutoML (PyCaret)](#ì‹¤í—˜-5-automl-pycaret)
  - [ì‹¤í—˜ 6: AutoML (Optuna)](#ì‹¤í—˜-6-automl-optuna)
- [âœï¸ ìš”ì•½ ì •ë¦¬](#ï¸-ìš”ì•½-ì •ë¦¬)

---

ê´€ì°°ëœ **ì—°ì†í˜• ë³€ìˆ˜**ë“¤ì— ëŒ€í•´ ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ ëª¨í˜•ì„ êµ¬í•œë’¤ ì í•©ë„ë¥¼ ì¸¡ì •í•´ ë‚´ëŠ” ë¶„ì„ ë°©ë²•ì´ë‹¤.

- **ë‹¨ìˆœíšŒê·€ë¶„ì„ (Simple Regression Analysis)**
    - í•˜ë‚˜ì˜ ì¢…ì†ë³€ìˆ˜ì™€ í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•œë‹¤.
    - `wx + b = yhat`

- **ë‹¤ì¤‘íšŒê·€ë¶„ì„ (Multiple Regression Analysis)**
   - í•˜ë‚˜ì˜ ì¢…ì†ë³€ìˆ˜ì™€ ì—¬ëŸ¬ ë…ë¦½ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ê·œëª…í•˜ê³ ì í•  ê²½ìš° ì‚¬ìš©í•œë‹¤.
   - `w1x1 + w2x2 + w3x3 + .... + b = yhat`

**ì˜ˆì‹œ**
- ì£¼íƒ ê°€ê²© ì˜ˆì¸¡
- ë§¤ì¶œì•¡ ì˜ˆì¸¡
- ì£¼ê°€ ì˜ˆì¸¡
- ì˜¨ë„ ì˜ˆì¸¡

**ëŒ€í‘œ íšŒê·€ ëª¨ë¸:**
- ìµœì†Œì œê³±ë²•(Ordinary Least Squares)ì„ í™œìš©í•œ `LinearRegression`
- ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì„ í™œìš©í•œ `SGDRegressor`
- ì„ í˜• íšŒê·€ ëª¨ë¸ì— L1, L2 ê·œì œë¥¼ ë”í•œ `Ridge`(ì œê³±), `Lasso`(ì ˆëŒ€ê°’), `ElasticNet`(ìƒí˜¸ë³´ì™„) ë“±

> **ëŒ€ì¡°**: ë¶„ë¥˜(Classification)ëŠ” ì£¼ì–´ì§„ íŠ¹ì„±ì— ë”°ë¼ ì–´ë–¤ ëŒ€ìƒì„ ìœ í•œí•œ ë²”ì£¼(íƒ€ê¹ƒê°’)ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë‹¤ë£¬ë‹¤.

---

### ğŸ§  ì°¸ê³  : ì„ í˜• ëª¨ë¸(Linear Models)

ì„ í˜• ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤. íšŒê·€ ë¶„ì„ì„ ìœ„í•œ ì„ í˜• ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$ \hat{y}(w,x) = w_0 + w_1 x_1 + ... + w_p x_p $$

- $x$: ì…ë ¥ ë°ì´í„°
- $w$: ëª¨ë¸ì´ í•™ìŠµí•  íŒŒë¼ë¯¸í„°
- $w_0$: í¸í–¥ (bias)
- $w_1$~$w_p$: ê°€ì¤‘ì¹˜ (weights)

<img src="https://t1.daumcdn.net/cfile/tistory/997E924F5CDBC1A628" alt="Linear Model">

> ì¶œì²˜: https://walkingwithus.tistory.com/606

**ì„ í˜• íšŒê·€ë¶„ì„ì˜ 4ê°€ì§€ ê¸°ë³¸ ê°€ì •**
- **ì„ í˜•ì„±**: ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì¢…ì†ë³€ìˆ˜ yì™€ ë…ë¦½ë³€ìˆ˜ x ê°„ì— ì„ í˜•ì„±ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.
- **ë…ë¦½ì„±**: ë…ë¦½ë³€ìˆ˜ x ê°„ì— ìƒê´€ê´€ê³„ ì—†ì´ ë…ë¦½ì„±ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤ (ë‹¤ì¤‘íšŒê·€ ì‹œ).
- **ë“±ë¶„ì‚°ì„±**: ì˜¤ì°¨ì˜ ë¶„ì‚°ì´ ì¼ì •í•´ì•¼ í•œë‹¤. ì¦‰, íŠ¹ì •í•œ íŒ¨í„´ ì—†ì´ ê³ ë¥´ê²Œ ë¶„í¬í•´ì•¼ í•œë‹¤.
- **ì •ê·œì„±**: ì”ì°¨(ì˜ˆì¸¡ì¹˜ì™€ ì‹¤ì œì¹˜ì˜ ì°¨ì´)ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc2F0oL%2FbtqFrUmcmlD%2FVjSZQF79ZksxaEj8Whmcz0%2Fimg.png)
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb3hByu%2FbtqFtIZDYBM%2FJDksb4QoJfgyy5BLFRRG4k%2Fimg.png)

---

## 1. ë°ì´í„° ìƒì„± ë° ë‹¨ìˆœ íšŒê·€ ğŸš€

`y = 2x + 5` ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ì˜ì˜ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , `LinearRegression` ëª¨ë¸ë¡œ íšŒê·€ì„ ì„ ì°¾ì•„ë³´ì•˜ë‹¤.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

np.random.seed(0)
w1 = 2  # xì˜ ê³„ìˆ˜
w0 = 5  # í¸í–¥ (yì ˆí¸)

# 0~4 ì‚¬ì´ì˜ ì‹¤ìˆ˜ê°’ 100ê°œ ìƒì„±
x = np.random.rand(100, 1) * 4 
# ë…¸ì´ì¦ˆ ì¶”ê°€
noise = np.random.rand(100, 1) * 2
y = w1 * x + w0 + noise

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
li_model = LinearRegression()
li_model.fit(x, y)

# ì˜ˆì¸¡
y_pred = li_model.predict(x)

# ê²°ê³¼ ì‹œê°í™”
plt.scatter(x, y, label='Actual Data')
plt.plot(x, y_pred, color='red', label='Regression Line')
plt.legend()
plt.show()

# í•™ìŠµëœ ê³„ìˆ˜ ë° ì ˆí¸ í™•ì¸
print(f"Rank: {li_model.rank_}")
print(f"Intercept (b): {li_model.intercept_}")
print(f"Coefficient (w): {li_model.coef_}")
print(f"R2 Score: {li_model.score(x, y)}")
```

---

## 2. íšŒê·€ ëª¨ë¸ í‰ê°€ ì§€í‘œ ğŸ§

ë¶„ë¥˜ ëª¨ë¸ê³¼ ë‹¬ë¦¬, íšŒê·€ ëª¨ë¸ì€ ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ë¯€ë¡œ ë‹¤ë¥¸ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•œë‹¤.

```python
import numpy as np
import pandas as pd
from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

np.random.seed(20)

# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
y_true = np.random.randint(low=1, high=999, size=700)
y_pred = y_true + np.round(np.random.random(700), decimals=1) * np.random.randint(low=-10, high=10, size=700)
```

### RÂ² Score (ê²°ì •ê³„ìˆ˜)
- ì‹¤ì œ ê°’ì˜ ë¶„ì‚° ëŒ€ë¹„ ì˜ˆì¸¡ ê°’ì˜ ë¶„ì‚° ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë©°, 0ì— ê°€ê¹ê±°ë‚˜ ìŒìˆ˜ì´ë©´ ëª¨ë¸ì´ ì˜ëª»ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.
- `1 - (sum(((y_true - y_pred)**2)) / sum((y_true - y_true.mean())**2))`
```python
print(f"R2 Score: {r2_score(y_true, y_pred)}")
```

### MSE (Mean Squared Error)
- ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í‰ê· ì„ ë‚¸ ê°’ì´ë‹¤.
- ì˜¤ì°¨ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ë©° ì‘ì„ìˆ˜ë¡ ì¢‹ë‹¤.
- $$ MSE = \frac{1}{n}\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^2 $$
```python
print(f"MSE: {mean_squared_error(y_true, y_pred)}")
```

### RMSE (Root Mean Squared Error)
- MSEì— ë£¨íŠ¸ë¥¼ ì”Œìš´ ê°’ìœ¼ë¡œ, MSEì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê³  ê°’ì˜ ì™œê³¡ì„ ì¤„ì—¬ì¤€ë‹¤.
```python
print(f"RMSE: {root_mean_squared_error(y_true, y_pred)}")
```

### MAE (Mean Absolute Error)
- ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ì— ëŒ€í•œ ì ˆëŒ€ê°’ì˜ í‰ê· ì´ë‹¤.
- ìŠ¤ì¼€ì¼ì— ì˜ì¡´ì ì´ë¼ëŠ” ë‹¨ì ì´ ìˆë‹¤.
- $$ MAE = \frac{1}{n}\sum_{i=1}^{n}\left|Y_{i}-\hat{Y}_{i}\right| $$
```python
print(f"MAE: {mean_absolute_error(y_true, y_pred)}")
```

### MAPE (Mean Absolute Percentage Error)
- ì˜¤ì°¨ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ ê°„ì˜ ì„±ëŠ¥ ë¹„êµì— ìœ ìš©í•˜ë‹¤.
- $$ \text{MAPE} = \frac{100\%}{n}\sum_{t=1}^{n}\left|\frac{A_t - F_t}{A_t}\right| $$
```python
# ì˜ˆì‹œ ë°ì´í„°
ì‹¤ì œ_ì•„íŒŒíŠ¸_ê°€ê²© = np.array([1000000, 2000000])
ì˜ˆì¸¡_ì•„íŒŒíŠ¸_ê°€ê²© = np.array([999900, 2000100])
ì‹¤ì œ_ê³¼ì¼_ê°€ê²©= np.array([500, 1000])
ì˜ˆì¸¡_ê³¼ì¼_ê°€ê²© = np.array([400, 1100])

print(f"ì•„íŒŒíŠ¸ MAPE: {mean_absolute_percentage_error(ì‹¤ì œ_ì•„íŒŒíŠ¸_ê°€ê²©, ì˜ˆì¸¡_ì•„íŒŒíŠ¸_ê°€ê²©)}")
print(f"ê³¼ì¼ MAPE: {mean_absolute_percentage_error(ì‹¤ì œ_ê³¼ì¼_ê°€ê²©, ì˜ˆì¸¡_ê³¼ì¼_ê°€ê²©)}")
```

---

## 3. ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ì‹¤ìŠµ ğŸ 

1990ë…„ëŒ€ ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•´ë³´ì•˜ë‹¤.

### ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰

```python
import pandas as pd
from sklearn.datasets import fetch_california_housing

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = pd.Series(housing.target, name='MedHouseVal')

caliDF = pd.concat([X, y], axis=1)
print(caliDF.corr()['MedHouseVal'].sort_values())
```

### ì‹¤í—˜ 1: Baseline ëª¨ë¸

ì „ì²˜ë¦¬ ì—†ì´ `LinearRegression` ëª¨ë¸ì„ ì ìš©í–ˆë‹¤.

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import root_mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=121)

li_model = LinearRegression()
li_model.fit(X_train, y_train)

y_pred = li_model.predict(X_test)

print(f"R2 Score: {li_model.score(X_test, y_test)}")
print(f"RMSE: {root_mean_squared_error(y_test, y_pred)}")

# ê²°ê³¼
# R2 Score: 0.6217862024682844
# RMSE: 0.7085757875825902
```

### ì‹¤í—˜ 2: Scaling

`RobustScaler`ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í–ˆë‹¤.

```python
from sklearn.preprocessing import RobustScaler

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=121)

# ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
li_model.fit(X_train_scaled, y_train)
y_pred = li_model.predict(X_test_scaled)

print(f"R2 Score (RobustScaler): {li_model.score(X_test_scaled, y_test)}")
print(f"RMSE (RobustScaler): {root_mean_squared_error(y_test, y_pred)}")

# ê²°ê³¼ (LinearRegressionì˜ ê²½ìš° Scalingì˜ ì˜í–¥ì´ ê±°ì˜ ì—†ìŒ)
# R2 Score (RobustScaler): 0.6217862024682844
# RMSE (RobustScaler): 0.7085757875825902
```

### ì‹¤í—˜ 3: Feature Selection

`SequentialFeatureSelector` (SFS)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ìš” íŠ¹ì„±ì„ ì„ íƒí–ˆë‹¤.

```python
from sklearn.feature_selection import SequentialFeatureSelector

li_model = LinearRegression()
sfs = SequentialFeatureSelector(li_model, n_features_to_select=5, direction='forward')
sfs.fit(X, y)

selected_features = list(sfs.get_feature_names_out())
print(f"Selected Features: {selected_features}")

# ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ì¬í•™ìŠµ
X_selected = X[selected_features]
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=121)

li_model.fit(X_train, y_train)
y_pred = li_model.predict(X_test)

print(f"R2 Score (SFS): {li_model.score(X_test, y_test)}")
print(f"RMSE (SFS): {root_mean_squared_error(y_test, y_pred)}")

# ê²°ê³¼ (SFS 5ê°œ ì„ ì •)
# R2 Score (SFS): 0.5522249378982188
# RMSE (SFS): 0.7709879235212429
```

### ì‹¤í—˜ 4: ê·œì œê°€ ìˆëŠ” ëª¨ë¸ (Ridge, Lasso)

ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ L1(Lasso), L2(Ridge) ê·œì œë¥¼ ì ìš©í–ˆë‹¤.

```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import RandomizedSearchCV
from scipy import stats

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=121)

param_dist = {"alpha": stats.reciprocal(1e-1, 1e1)}

# Ridge
ridge = Ridge()
ri_model_search = RandomizedSearchCV(ridge, param_distributions=param_dist, n_iter=70, scoring="neg_root_mean_squared_error", cv=5, random_state=42)
ri_model_search.fit(X_train, y_train)
ri_model = ri_model_search.best_estimator_
y_pred_ridge = ri_model.predict(X_test)

# Lasso
lasso = Lasso()
la_model_search = RandomizedSearchCV(lasso, param_distributions=param_dist, n_iter=70, scoring="neg_root_mean_squared_error", cv=5, random_state=42)
la_model_search.fit(X_train, y_train)
la_model = la_model_search.best_estimator_
y_pred_lasso = la_model.predict(X_test)

print(f"Ridge Best Alpha: {ri_model.alpha}")
print(f"Lasso Best Alpha: {la_model.alpha}")
print(f"Ridge R2: {ri_model.score(X_test, y_test)}")
print(f"Lasso R2: {la_model.score(X_test, y_test)}")
print(f"Ridge RMSE: {root_mean_squared_error(y_test, y_pred_ridge)}")
print(f"Lasso RMSE: {root_mean_squared_error(y_test, y_pred_lasso)}")

# ê²°ê³¼
# Ridge R2: 0.6209...
# Lasso R2: 0.6117...
```

### ì‹¤í—˜ 5: AutoML (PyCaret)

PyCaretì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë¹„êµí•˜ê³  ìµœì ì˜ ëª¨ë¸ì„ ì°¾ì•˜ë‹¤.

```python
# !pip install pycaret
from pycaret.regression import *

# ë°ì´í„° ì¤€ë¹„
s = setup(caliDF, target='MedHouseVal', session_id=123, preprocess=False, verbose=False)

# ëª¨ë¸ ë¹„êµ
best_model = compare_models()
print(best_model)

# ëª¨ë¸ ìƒì„± ë° í‰ê°€
# ì˜ˆ: catboost = create_model('catboost')
# evaluate_model(catboost)
```

### ì‹¤í—˜ 6: AutoML (Optuna)

Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ `ExtraTreesRegressor` ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í–ˆë‹¤.

```python
# !pip install optuna
import optuna
from sklearn.ensemble import ExtraTreesRegressor

def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ ì„¤ì •
    n_estimators = trial.suggest_int('n_estimators', 50, 200)
    max_depth = trial.suggest_int('max_depth', 5, 50)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)

    # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
    model = ExtraTreesRegressor(n_estimators=n_estimators,
                                  max_depth=max_depth,
                                  min_samples_split=min_samples_split,
                                  random_state=42)
    model.fit(X_train, y_train)

    # ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
    y_pred = model.predict(X_test)
    rmse = root_mean_squared_error(y_test, y_pred)
    return rmse

# ìµœì í™” ì‹¤í–‰
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30)

print(f'Best parameters: {study.best_params}')
print(f'Best RMSE: {study.best_value}')
```

---

### âœï¸ ìš”ì•½ ì •ë¦¬
íšŒê·€ ë¶„ì„ì€ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€ë„í•™ìŠµ ë°©ë²•ì´ë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ì€ RÂ², RMSE ë“± ë‹¤ì–‘í•œ ì§€í‘œë¡œ í‰ê°€í•˜ë©°, ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ì§€í‘œë§Œ ë³´ëŠ” ê²ƒì€ ìœ„í—˜í•˜ë‹¤. ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ì‹¤ìŠµì„ í†µí•´ ìŠ¤ì¼€ì¼ë§, íŠ¹ì„± ì„ íƒ, ê·œì œ(Ridge, Lasso), AutoML ë“± ë‹¤ì–‘í•œ ê¸°ë²•ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•´ë‚˜ê°€ëŠ” ê³¼ì •ì„ í•™ìŠµí–ˆë‹¤. íŠ¹íˆ ê·œì œëŠ” ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° íš¨ê³¼ì ì´ì—ˆë‹¤.

```