# 🌳 결정트리 앙상블 (Ensemble)

## 🎯 목차
- [결정트리 앙상블](#-결정트리-앙상블-ensemble-학습)
- [1. Simple Voting 🗳️](#1-simple-voting-️)
- [2. Bagging - Random Forest 🌳](#2-bagging---random-forest-)
- [3. Boosting ✨](#3-boosting-)
  - [가. GBM (Gradient Boosting Machine)](#가-gbm-gradient-boosting-machine)
  - [나. XGBoost (eXtra Gradient Boosting)](#나-xgboost-extra-gradient-boosting)
  - [다. LightGBM](#다-lightgbm)
- [✍️ 요약 정리](#️-요약-정리)

---

머신러닝 모델을 여러 개 연결하여 더 강력한 모델을 만드는 기법이다.

![](https://ars.els-cdn.com/content/image/1-s2.0-S1566253520303195-gr1.jpg)

---

## 1. Simple Voting 🗳️

- **Hard Voting**: 다수의 분류기들 간에 다수결로 최종 클래스를 선정하는 방법이다.
- **Soft Voting**: 다수의 분류기들이 각각 분류한 확률값들을 기반으로 평균을 내어, 더 높은 확률값을 갖는 클래스로 최종 선정하는 방법이다. 일반적으로 성능은 Soft voting 방법이 더 우수하다고 알려져 있다.

![](https://miro.medium.com/max/700/0*ikXrXKWDoTRi18Tm.png)

### 실습: 유방암 데이터 분류

유방암 진단 데이터셋을 사용하여 Logistic Regression, KNN, Decision Tree 세 가지 모델을 VotingClassifier로 묶어 예측을 수행했다.

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
cancer = load_breast_cancer()
df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
df['category'] = cancer.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], random_state=121, test_size=0.2)

# 개별 모델 정의
lr_clf = LogisticRegression(max_iter=10000)
knn_clf = KNeighborsClassifier()
dt_clf = DecisionTreeClassifier()

# VotingClassifier 생성 (Soft Voting)
# weights 파라미터로 각 모델의 예측에 가중치를 부여할 수 있다.
vo_clf = VotingClassifier([('LR', lr_clf), ('KNN', knn_clf), ('DT', dt_clf)], voting="soft", weights=[0.2, 0.7, 0.1])

# 모델 학습 및 평가
vo_clf.fit(X_train, y_train)
print(f"Soft Voting Accuracy: {vo_clf.score(X_test, y_test)}") # Soft Voting Accuracy: 0.9473684210526315

# Hard Voting
vo_clf_hard = VotingClassifier([('LR', lr_clf), ('KNN', knn_clf), ('DT', dt_clf)], voting="hard")
vo_clf_hard.fit(X_train, y_train)
print(f"Hard Voting Accuracy: {vo_clf_hard.score(X_test, y_test)}") # Hard Voting Accuracy: 0.9736842105263158
```

---

## 2. Bagging - Random Forest 🌳

**배깅(Bagging, Bootstrap Aggregating)** 은 같은 모델 여러 개에 데이터의 일부를 복원추출(Bootstrap)하여 병렬로 학습시키는 방식이다. 각 모델이 약간씩 다른 데이터 분포를 학습하므로 전체 모델의 과적합을 방지하는 효과가 있다. **Random Forest**가 대표적인 배깅 모델이다.

```python
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

# RandomForest 모델 생성 및 학습
rf_clf = RandomForestClassifier(random_state=121, n_estimators=100) # 100개의 Decision Tree 사용
rf_clf.fit(X_train, y_train)

print(f"Random Forest Accuracy: {rf_clf.score(X_test, y_test)}") # Random Forest Accuracy: 0.9824561403508771

# 특성 중요도 시각화
def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.figure(figsize=(10, 8))
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Feature Importances")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)
    plt.show()

plot_feature_importances_cancer(rf_clf)
```

---

## 3. Boosting ✨

**부스팅(Boosting)** 은 여러 개의 약한 학습기(Weak learner)를 순차적으로 학습시켜, 이전 모델의 오류를 보완해가며 점차 강력한 모델을 만들어가는 방식이다. 학습 시간이 상대적으로 오래 걸릴 수 있지만, 일반적으로 배깅보다 높은 성능을 보인다.

### 가. GBM (Gradient Boosting Machine)

경사하강법(Gradient Descent)에 기반하여 이전 모델의 오차를 보완하는 방식으로, 잘못 분류된 데이터에 가중치를 부여하며 순차적으로 다음 모델을 학습시킨다.

```python
from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(n_estimators=1000, verbose=True)
gb_clf.fit(X_train, y_train)

print(f"GBM Accuracy: {gb_clf.score(X_test, y_test)}") # GBM Accuracy: 0.9824561403508771

# 특성 중요도 시각화
# 부스팅 모델은 잘못 맞춘 특성을 강조해서 학습하므로, 특성 중요도가 극단적으로 나타날 수 있다.
plot_feature_importances_cancer(gb_clf)
```

### 나. XGBoost (eXtra Gradient Boosting)

GBM의 단점인 느린 속도를 병렬 처리 지원(GPU) 등으로 개선한 알고리즘이다. 과적합 방지를 위한 규제, 조기 중단(Early Stopping), 내장 교차 검증 등 다양한 부가기능을 제공하여 현재 많은 사랑을 받고 있다.

```python
# !pip install xgboost
from xgboost import XGBClassifier
from xgboost import plot_importance

# XGBoost 모델 생성 및 학습
# eval_set으로 검증 세트를 지정하고, early_stopping_rounds로 조기 중단 설정
xgbc = XGBClassifier(n_estimators=1000, early_stopping_rounds=10, eval_metric='logloss', verbose=True)
xgbc.fit(X_train, y_train, eval_set=[(X_test, y_test)])

print(f"XGBoost Accuracy: {xgbc.score(X_test, y_test)}") # XGBoost Accuracy: 0.9824561403508771

# 특성 중요도 시각화
plot_importance(xgbc, title='Feature importance for cancer data')
plt.show()
```

### 다. LightGBM

XGBoost보다 더 빠른 속도와 적은 메모리 사용량을 자랑하는 부스팅 모델이다. 대용량 데이터 처리에 유리하며, **리프 중심 트리 분할(Leaf Wise)** 방식을 사용하여 비대칭적인 트리를 생성하며 예측 오류를 최소화한다. 다만, 데이터 수가 너무 적을 경우 과적합이 발생하기 쉽다.

```python
# !pip install lightgbm
import lightgbm
from lightgbm import LGBMClassifier

# LightGBM은 scikit-learn이 아닌 별도의 라이브러리이다.
print(f"LightGBM Version: {lightgbm.__version__}")
```

---

### ✍️ 요약 정리
앙상블 학습은 여러 모델을 결합하여 단일 모델보다 강력한 성능을 내는 기법이다. **Voting**은 여러 다른 모델의 예측을 종합하고, **Bagging(Random Forest)** 은 같은 모델 여러 개를 병렬로 학습시켜 과적합을 줄인다. **Boosting(GBM, XGBoost, LightGBM)** 은 모델을 순차적으로 학습하며 이전 모델의 실수를 보완해나가므로 일반적으로 성능이 가장 뛰어나다. 특히 XGBoost와 LightGBM은 빠른 속도와 높은 예측 성능으로 캐글 등 데이터 경진대회에서 널리 사용된다. 성공! 🎉
