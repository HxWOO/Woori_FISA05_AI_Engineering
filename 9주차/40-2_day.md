# ğŸŒ³ ê²°ì •íŠ¸ë¦¬ ì•™ìƒë¸” (Ensemble)

## ğŸ¯ ëª©ì°¨
- [ê²°ì •íŠ¸ë¦¬ ì•™ìƒë¸”](#-ê²°ì •íŠ¸ë¦¬-ì•™ìƒë¸”-ensemble-í•™ìŠµ)
- [1. Simple Voting ğŸ—³ï¸](#1-simple-voting-ï¸)
- [2. Bagging - Random Forest ğŸŒ³](#2-bagging---random-forest-)
- [3. Boosting âœ¨](#3-boosting-)
  - [ê°€. GBM (Gradient Boosting Machine)](#ê°€-gbm-gradient-boosting-machine)
  - [ë‚˜. XGBoost (eXtra Gradient Boosting)](#ë‚˜-xgboost-extra-gradient-boosting)
  - [ë‹¤. LightGBM](#ë‹¤-lightgbm)
- [âœï¸ ìš”ì•½ ì •ë¦¬](#ï¸-ìš”ì•½-ì •ë¦¬)

---

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì—¬ëŸ¬ ê°œ ì—°ê²°í•˜ì—¬ ë” ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê¸°ë²•ì´ë‹¤.

![](https://ars.els-cdn.com/content/image/1-s2.0-S1566253520303195-gr1.jpg)

---

## 1. Simple Voting ğŸ—³ï¸

- **Hard Voting**: ë‹¤ìˆ˜ì˜ ë¶„ë¥˜ê¸°ë“¤ ê°„ì— ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… í´ë˜ìŠ¤ë¥¼ ì„ ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- **Soft Voting**: ë‹¤ìˆ˜ì˜ ë¶„ë¥˜ê¸°ë“¤ì´ ê°ê° ë¶„ë¥˜í•œ í™•ë¥ ê°’ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í‰ê· ì„ ë‚´ì–´, ë” ë†’ì€ í™•ë¥ ê°’ì„ ê°–ëŠ” í´ë˜ìŠ¤ë¡œ ìµœì¢… ì„ ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì€ Soft voting ë°©ë²•ì´ ë” ìš°ìˆ˜í•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.

![](https://miro.medium.com/max/700/0*ikXrXKWDoTRi18Tm.png)

### ì‹¤ìŠµ: ìœ ë°©ì•” ë°ì´í„° ë¶„ë¥˜

ìœ ë°©ì•” ì§„ë‹¨ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ Logistic Regression, KNN, Decision Tree ì„¸ ê°€ì§€ ëª¨ë¸ì„ VotingClassifierë¡œ ë¬¶ì–´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í–ˆë‹¤.

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ë°ì´í„° ë¡œë“œ
cancer = load_breast_cancer()
df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
df['category'] = cancer.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], random_state=121, test_size=0.2)

# ê°œë³„ ëª¨ë¸ ì •ì˜
lr_clf = LogisticRegression(max_iter=10000)
knn_clf = KNeighborsClassifier()
dt_clf = DecisionTreeClassifier()

# VotingClassifier ìƒì„± (Soft Voting)
# weights íŒŒë¼ë¯¸í„°ë¡œ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆë‹¤.
vo_clf = VotingClassifier([('LR', lr_clf), ('KNN', knn_clf), ('DT', dt_clf)], voting="soft", weights=[0.2, 0.7, 0.1])

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
vo_clf.fit(X_train, y_train)
print(f"Soft Voting Accuracy: {vo_clf.score(X_test, y_test)}") # Soft Voting Accuracy: 0.9473684210526315

# Hard Voting
vo_clf_hard = VotingClassifier([('LR', lr_clf), ('KNN', knn_clf), ('DT', dt_clf)], voting="hard")
vo_clf_hard.fit(X_train, y_train)
print(f"Hard Voting Accuracy: {vo_clf_hard.score(X_test, y_test)}") # Hard Voting Accuracy: 0.9736842105263158
```

---

## 2. Bagging - Random Forest ğŸŒ³

**ë°°ê¹…(Bagging, Bootstrap Aggregating)** ì€ ê°™ì€ ëª¨ë¸ ì—¬ëŸ¬ ê°œì— ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ë³µì›ì¶”ì¶œ(Bootstrap)í•˜ì—¬ ë³‘ë ¬ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì´ë‹¤. ê° ëª¨ë¸ì´ ì•½ê°„ì”© ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ ì „ì²´ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. **Random Forest**ê°€ ëŒ€í‘œì ì¸ ë°°ê¹… ëª¨ë¸ì´ë‹¤.

```python
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

# RandomForest ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
rf_clf = RandomForestClassifier(random_state=121, n_estimators=100) # 100ê°œì˜ Decision Tree ì‚¬ìš©
rf_clf.fit(X_train, y_train)

print(f"Random Forest Accuracy: {rf_clf.score(X_test, y_test)}") # Random Forest Accuracy: 0.9824561403508771

# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.figure(figsize=(10, 8))
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Feature Importances")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)
    plt.show()

plot_feature_importances_cancer(rf_clf)
```

---

## 3. Boosting âœ¨

**ë¶€ìŠ¤íŒ…(Boosting)** ì€ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµê¸°(Weak learner)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ, ì´ì „ ëª¨ë¸ì˜ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•´ê°€ë©° ì ì°¨ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ê°€ëŠ” ë°©ì‹ì´ë‹¤. í•™ìŠµ ì‹œê°„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë°°ê¹…ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

### ê°€. GBM (Gradient Boosting Machine)

ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì— ê¸°ë°˜í•˜ì—¬ ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.

```python
from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(n_estimators=1000, verbose=True)
gb_clf.fit(X_train, y_train)

print(f"GBM Accuracy: {gb_clf.score(X_test, y_test)}") # GBM Accuracy: 0.9824561403508771

# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
# ë¶€ìŠ¤íŒ… ëª¨ë¸ì€ ì˜ëª» ë§ì¶˜ íŠ¹ì„±ì„ ê°•ì¡°í•´ì„œ í•™ìŠµí•˜ë¯€ë¡œ, íŠ¹ì„± ì¤‘ìš”ë„ê°€ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤.
plot_feature_importances_cancer(gb_clf)
```

### ë‚˜. XGBoost (eXtra Gradient Boosting)

GBMì˜ ë‹¨ì ì¸ ëŠë¦° ì†ë„ë¥¼ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›(GPU) ë“±ìœ¼ë¡œ ê°œì„ í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê·œì œ, ì¡°ê¸° ì¤‘ë‹¨(Early Stopping), ë‚´ì¥ êµì°¨ ê²€ì¦ ë“± ë‹¤ì–‘í•œ ë¶€ê°€ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ í˜„ì¬ ë§ì€ ì‚¬ë‘ì„ ë°›ê³  ìˆë‹¤.

```python
# !pip install xgboost
from xgboost import XGBClassifier
from xgboost import plot_importance

# XGBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
# eval_setìœ¼ë¡œ ê²€ì¦ ì„¸íŠ¸ë¥¼ ì§€ì •í•˜ê³ , early_stopping_roundsë¡œ ì¡°ê¸° ì¤‘ë‹¨ ì„¤ì •
xgbc = XGBClassifier(n_estimators=1000, early_stopping_rounds=10, eval_metric='logloss', verbose=True)
xgbc.fit(X_train, y_train, eval_set=[(X_test, y_test)])

print(f"XGBoost Accuracy: {xgbc.score(X_test, y_test)}") # XGBoost Accuracy: 0.9824561403508771

# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
plot_importance(xgbc, title='Feature importance for cancer data')
plt.show()
```

### ë‹¤. LightGBM

XGBoostë³´ë‹¤ ë” ë¹ ë¥¸ ì†ë„ì™€ ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìë‘í•˜ëŠ” ë¶€ìŠ¤íŒ… ëª¨ë¸ì´ë‹¤. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ìœ ë¦¬í•˜ë©°, **ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í• (Leaf Wise)** ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ëŒ€ì¹­ì ì¸ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ë©° ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•œë‹¤. ë‹¤ë§Œ, ë°ì´í„° ìˆ˜ê°€ ë„ˆë¬´ ì ì„ ê²½ìš° ê³¼ì í•©ì´ ë°œìƒí•˜ê¸° ì‰½ë‹¤.

```python
# !pip install lightgbm
import lightgbm
from lightgbm import LGBMClassifier

# LightGBMì€ scikit-learnì´ ì•„ë‹Œ ë³„ë„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.
print(f"LightGBM Version: {lightgbm.__version__}")
```

---

### âœï¸ ìš”ì•½ ì •ë¦¬
ì•™ìƒë¸” í•™ìŠµì€ ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ê¸°ë²•ì´ë‹¤. **Voting**ì€ ì—¬ëŸ¬ ë‹¤ë¥¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì¢…í•©í•˜ê³ , **Bagging(Random Forest)** ì€ ê°™ì€ ëª¨ë¸ ì—¬ëŸ¬ ê°œë¥¼ ë³‘ë ¬ë¡œ í•™ìŠµì‹œì¼œ ê³¼ì í•©ì„ ì¤„ì¸ë‹¤. **Boosting(GBM, XGBoost, LightGBM)** ì€ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ë©° ì´ì „ ëª¨ë¸ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ì™„í•´ë‚˜ê°€ë¯€ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ë›°ì–´ë‚˜ë‹¤. íŠ¹íˆ XGBoostì™€ LightGBMì€ ë¹ ë¥¸ ì†ë„ì™€ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ìœ¼ë¡œ ìºê¸€ ë“± ë°ì´í„° ê²½ì§„ëŒ€íšŒì—ì„œ ë„ë¦¬ ì‚¬ìš©ëœë‹¤. ì„±ê³µ! ğŸ‰
