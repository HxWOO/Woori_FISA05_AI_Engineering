---

## 🚀 미니 프로젝트: 채용 공고 분석 및 시각화

### 🎯 목표
채용 사이트(예: 사람인)에서 '데이터 분석가' 직무의 채용 공고를 크롤링하여, 요구 기술 스택, 지역별 공고 수 등을 분석하고 시각화합니다.

### 📝 단계별 가이드

#### STEP 1: 데이터 수집 (Crawling)
- **To-Do:**
    1.  `Playwright`를 사용하여 '사람인' 사이트에 접속합니다.
    2.  '데이터 분석가' 키워드로 직무를 검색합니다.
    3.  개발자 도구를 활용해 공고 목록의 HTML 구조를 분석합니다.
        - **Hint:** 각 공고는 `div`와 같은 태그로 감싸져 있으며, 그 안에 회사명, 공고 제목, 근무지, 요구 기술 등의 정보가 포함되어 있습니다.
    4.  한 페이지 내의 모든 공고에서 아래 정보를 추출하는 코드를 작성합니다.
        - `회사명`, `공고 제목`, `근무 지역`, `요구 기술`
    5.  다음 페이지 버튼을 클릭하거나 URL을 변경하여 2~3 페이지의 데이터를 추가로 수집합니다.
    6.  수집된 데이터를 `List[Dict]` 형태로 만든 후 `JSON` 파일로 저장합니다. (예: `saramin_jobs.json`)

#### STEP 2: 데이터 정제 및 가공 (Processing)
- **To-Do:**
    1.  `Pandas`를 이용해 `saramin_jobs.json` 파일을 `DataFrame`으로 불러옵니다.
    2.  **요구 기술** 컬럼의 텍스트 데이터를 처리합니다.
        - **Hint:** 쉼표(`,`), 슬래시(`/`) 등을 기준으로 문자열을 분리하여 기술 키워드 리스트를 만듭니다. (예: `'Python, SQL, AWS'` -> `['Python', 'SQL', 'AWS']`)
        - 모든 키워드를 소문자로 변환하고, 공백을 제거하여 통일성을 높입니다.
    3.  **근무 지역** 컬럼도 시/도 단위로 간소화합니다. (예: `'서울 강남구'` -> `'서울'`)
    4.  처리된 데이터를 새로운 `CSV` 파일로 저장합니다. (예: `saramin_jobs_processed.csv`)

#### STEP 3: 분석 및 시각화 (Visualization)
- **To-Do:**
    1.  정제된 `CSV` 파일을 `DataFrame`으로 불러옵니다.
    2.  **기술 스택 빈도 분석**:
        - 모든 공고의 기술 키워드를 하나의 리스트로 합친 후, 각 키워드의 빈도를 계산합니다. (`value_counts()` 활용)
        - `Plotly`를 사용하여 가장 많이 요구되는 기술 상위 10개를 **막대그래프**로 시각화합니다.
        - **(Optional)** `WordCloud` 라이브러리를 설치하여 기술 스택을 **워드 클라우드**로 시각화해봅니다.
    3.  **지역별 공고 수 분석**:
        - `groupby()`와 `size()`를 이용해 지역별 공고 수를 계산합니다.
        - `Plotly`를 사용하여 지역별 공고 수 분포를 **파이 차트**로 시각화합니다.
    4.  생성된 모든 차트를 이미지 파일(예: `.png`)로 저장합니다.

#### STEP 4: 문서화 (Documentation)
- **To-Do:**
    1.  이 `README.md` 파일을 수정하여 미니 프로젝트의 진행 과정, 최종 결과물(차트 이미지 포함), 그리고 프로젝트를 통해 배운 점이나 어려웠던 점을 상세히 기록합니다.